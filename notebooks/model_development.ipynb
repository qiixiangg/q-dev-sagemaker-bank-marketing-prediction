{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Marketing Campaign Success Prediction - Model Development\n",
    "\n",
    "This notebook demonstrates a comprehensive Data Science Life Cycle (DSLC) for predicting bank marketing campaign success. The goal is to predict whether a client will subscribe to a term deposit.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The data is related to direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to assess if the product (bank term deposit) would be subscribed ('yes') or not ('no').\n",
    "\n",
    "### Features\n",
    "\n",
    "#### Bank Client Data\n",
    "1. age: numeric\n",
    "2. job: type of job\n",
    "3. marital: marital status\n",
    "4. education: education level\n",
    "5. default: has credit in default?\n",
    "6. housing: has housing loan?\n",
    "7. loan: has personal loan?\n",
    "\n",
    "#### Campaign Data\n",
    "8. contact: contact communication type\n",
    "9. month: last contact month of year\n",
    "10. day_of_week: last contact day of the week\n",
    "11. duration: last contact duration, in seconds\n",
    "12. campaign: number of contacts performed during this campaign\n",
    "13. pdays: number of days that passed by after the client was last contacted\n",
    "14. previous: number of contacts performed before this campaign\n",
    "15. poutcome: outcome of the previous marketing campaign\n",
    "\n",
    "#### Economic Context Data\n",
    "16. emp.var.rate: employment variation rate - quarterly indicator\n",
    "17. cons.price.idx: consumer price index - monthly indicator\n",
    "18. cons.conf.idx: consumer confidence index - monthly indicator\n",
    "19. euribor3m: euribor 3 month rate - daily indicator\n",
    "20. nr.employed: number of employees - quarterly indicator\n",
    "\n",
    "#### Target Variable\n",
    "- y: has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "\n",
    "## DSLC Steps\n",
    "\n",
    "1. Data Collection & Loading\n",
    "2. Data Exploration & Analysis\n",
    "3. Data Preprocessing\n",
    "4. Feature Engineering\n",
    "5. Model Development & Training\n",
    "6. Model Evaluation\n",
    "7. Model Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading\n",
    "\n",
    "First, let's set up MLflow and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, confusion_matrix\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('classic')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Download data if it doesn't exist\n",
    "data_dir = '../data/raw'\n",
    "data_file = 'bank-additional-full.csv'\n",
    "data_path = os.path.join(data_dir, data_file)\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\"\n",
    "    r = requests.get(url)\n",
    "    zip_path = os.path.join(data_dir, \"bank-additional.zip\")\n",
    "    \n",
    "    # Save the zip file\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    \n",
    "    # Create a temporary directory for extraction\n",
    "    temp_extract_dir = os.path.join(data_dir, 'temp')\n",
    "    os.makedirs(temp_extract_dir, exist_ok=True)\n",
    "    \n",
    "    # Unzip the file to temporary directory\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temp_extract_dir)\n",
    "    \n",
    "    # Move the required file to the target location\n",
    "    extracted_file = os.path.join(temp_extract_dir, data_file)\n",
    "    if os.path.exists(extracted_file):\n",
    "        shutil.move(extracted_file, data_path)\n",
    "    else:\n",
    "        # Search for the file in subdirectories if not found in root\n",
    "        for root, _, files in os.walk(temp_extract_dir):\n",
    "            if data_file in files:\n",
    "                full_path = os.path.join(root, data_file)\n",
    "                shutil.move(full_path, data_path)\n",
    "                break\n",
    "    \n",
    "    # Clean up\n",
    "    shutil.rmtree(temp_extract_dir)\n",
    "    os.remove(zip_path)\n",
    "    print(\"Dataset downloaded and extracted successfully!\")\n",
    "else:\n",
    "    print(\"Dataset already exists!\")\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path, sep=';')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration & Analysis\n",
    "\n",
    "Let's explore our dataset to understand:\n",
    "- Data quality (missing values, duplicates)\n",
    "- Feature distributions\n",
    "- Target distribution\n",
    "- Feature relationships with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing Values:\")\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTarget Distribution:\")\n",
    "display(df['y'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x='y')\n",
    "plt.title('Target Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze numeric features\n",
    "numeric_features = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', \n",
    "                   'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, feature in enumerate(numeric_features, 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.boxplot(data=df, x='y', y=feature)\n",
    "    plt.title(f'{feature} by Target')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features\n",
    "categorical_features = ['job', 'marital', 'education', 'default', 'housing', \n",
    "                       'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "\n",
    "# Create pairs of features for side-by-side display\n",
    "feature_pairs = [(categorical_features[i], categorical_features[i+1]) \n",
    "                 for i in range(0, len(categorical_features), 2)]\n",
    "\n",
    "# If there's an odd number of features, add the last one separately\n",
    "if len(categorical_features) % 2 != 0:\n",
    "    feature_pairs.append((categorical_features[-1], None))\n",
    "\n",
    "for pair in feature_pairs:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    \n",
    "    # Plot first feature\n",
    "    df_pct = df.groupby(pair[0])['y'].value_counts(normalize=True).unstack()\n",
    "    df_pct['yes'].sort_values().plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title(f'Success Rate by {pair[0]}')\n",
    "    axes[0].set_ylabel('Success Rate')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot second feature if it exists\n",
    "    if pair[1] is not None:\n",
    "        df_pct = df.groupby(pair[1])['y'].value_counts(normalize=True).unstack()\n",
    "        df_pct['yes'].sort_values().plot(kind='bar', ax=axes[1])\n",
    "        axes[1].set_title(f'Success Rate by {pair[1]}')\n",
    "        axes[1].set_ylabel('Success Rate')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        # Hide the second subplot if there's no second feature\n",
    "        axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Based on our analysis, let's preprocess the data:\n",
    "1. Convert target to numeric\n",
    "2. Encode categorical features\n",
    "3. Scale numeric features\n",
    "4. Split data into train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target to numeric\n",
    "df['y'] = (df['y'] == 'yes').astype(int)\n",
    "\n",
    "# Initialize label encoders for categorical features\n",
    "label_encoders = {}\n",
    "for feature in categorical_features:\n",
    "    label_encoders[feature] = LabelEncoder()\n",
    "    df[feature] = label_encoders[feature].fit_transform(df[feature])\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y']\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_val[numeric_features] = scaler.transform(X_val[numeric_features])\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development & Training\n",
    "\n",
    "We'll use XGBoost for this binary classification task and perform:\n",
    "1. Cross-validation training\n",
    "2. Hyperparameter tuning\n",
    "3. Final model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model parameters\n",
    "base_params = {\n",
    "    'max_depth': 5,\n",
    "    'eta': 0.5,\n",
    "    'alpha': 2.5,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 3,\n",
    "    'tree_method': 'auto'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost classifier\n",
    "model_cv = xgb.XGBClassifier(**base_params)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(model_cv, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f\"CV Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'eta': [0.1, 0.3, 0.5],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "model_tune = xgb.XGBClassifier(**base_params)\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_tune,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Update base_params with best parameters\n",
    "base_params.update(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to DMatrix format\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "# Train the model\n",
    "model = xgb.train(\n",
    "    params=base_params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=150,\n",
    "    evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "Let's evaluate our model using:\n",
    "1. ROC-AUC score\n",
    "2. Confusion matrix\n",
    "3. Feature importance\n",
    "4. Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "y_pred_proba = model.predict(dtest)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Test AUC: {auc_score:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "importance_scores = model.get_score(importance_type='gain')\n",
    "importance_df = pd.DataFrame(\n",
    "    list(importance_scores.items()),\n",
    "    columns=['Feature', 'Importance']\n",
    ").sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance_df.head(10), x='Importance', y='Feature')\n",
    "plt.title('Top 10 Feature Importance (Gain)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Export\n",
    "\n",
    "Save the model and necessary artifacts for production deployment. These files will be used by the ML Engineering team for model serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "os.makedirs(\"../model\", exist_ok=True)\n",
    "model.save_model('../model/xgboost_model.json')\n",
    "\n",
    "# Save feature names and other metadata\n",
    "model_metadata = {\n",
    "    'feature_names': X.columns.tolist(),\n",
    "    'numeric_features': numeric_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'model_params': base_params,\n",
    "    'prediction_threshold': 0.5,\n",
    "    'metrics': {\n",
    "        'auc_score': auc_score\n",
    "    },\n",
    "    'label_mapping': {\n",
    "        feature: dict(zip(encoder.classes_, range(len(encoder.classes_))))\n",
    "        for feature, encoder in label_encoders.items()\n",
    "    },\n",
    "    'scaler_params': {\n",
    "        'mean': scaler.mean_.tolist(),\n",
    "        'scale': scaler.scale_.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../model/model_metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_metadata, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
